# ğŸš€ Training Implementation Flow & Architecture Guide

**Smart ML Assistant - Complete Training Pipeline Documentation**

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Architecture Diagram](#architecture-diagram)
3. [Component Flow](#component-flow)
4. [Detailed Training Flow](#detailed-training-flow)
5. [Code Implementation](#code-implementation)
6. [Integration Points](#integration-points)
7. [Data Flow](#data-flow)
8. [API Endpoints](#api-endpoints)

---

## ğŸ¯ Overview

The Smart ML Assistant implements a **complete end-to-end ML training pipeline** that integrates:
- Multi-source dataset discovery (Kaggle + HuggingFace)
- AI-powered model recommendations
- AutoML training with real-time progress tracking
- Deployment-ready models

### Key Features:
- âœ… Automatic dataset inspection with schema detection
- âœ… Auto-detection of target columns
- âœ… Real AutoML training using AutoGluon
- âœ… Server-Sent Events (SSE) for live progress updates
- âœ… Fallback to simulation mode if AutoGluon unavailable
- âœ… Production-safe (works with ephemeral filesystems)

---

## ğŸ—ï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Chat.tsx   â”‚  â”‚ DatasetCard  â”‚  â”‚  useTraining â”‚             â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚     Hook     â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â”‚ User Query       â”‚ Click Train      â”‚ SSE Connection
          â”‚                  â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BACKEND API LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  messages.py     â”‚  â”‚  datasets.py     â”‚  â”‚   automl.py     â”‚  â”‚
â”‚  â”‚  /api/messages   â”‚  â”‚  /api/datasets   â”‚  â”‚  /api/automl    â”‚  â”‚
â”‚  â”‚    /chat         â”‚  â”‚    /inspect      â”‚  â”‚    /train       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                    â”‚                     â”‚
            â”‚                    â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       SERVICE LAYER                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ml_orchestrator  â”‚  â”‚ dataset_download â”‚  â”‚  gemini_service â”‚  â”‚
â”‚  â”‚   .py            â”‚  â”‚   _service.py    â”‚  â”‚      .py        â”‚  â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                 â”‚  â”‚
â”‚  â”‚ - recommend()    â”‚  â”‚ - search()       â”‚  â”‚ - analyze()     â”‚  â”‚
â”‚  â”‚ - make_decision()â”‚  â”‚ - download()     â”‚  â”‚ - optimize()    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                    â”‚                     â”‚
            â”‚                    â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EXTERNAL SERVICES                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Kaggle     â”‚  â”‚ HuggingFace  â”‚  â”‚    Gemini    â”‚             â”‚
â”‚  â”‚     API      â”‚  â”‚     Hub      â”‚  â”‚     API      â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                    â”‚                     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA STORAGE                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      MongoDB                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚  â”‚   datasets   â”‚  â”‚    models    â”‚  â”‚   messages   â”‚       â”‚   â”‚
â”‚  â”‚  â”‚  collection  â”‚  â”‚  collection  â”‚  â”‚  collection  â”‚       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  File System / Storage                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚  â”‚  CSV Files   â”‚  â”‚ Trained      â”‚  â”‚  AutoGluon   â”‚       â”‚   â”‚
â”‚  â”‚  â”‚  (datasets)  â”‚  â”‚  Models      â”‚  â”‚   Cache      â”‚       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Component Flow

### 1. Dataset Discovery Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚ "Find sentiment analysis datasets"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ messages.py: /api/messages/chat             â”‚
â”‚                                              â”‚
â”‚ 1. Analyze query with Gemini                â”‚
â”‚ 2. Detect: needs_kaggle_search = True       â”‚
â”‚ 3. Extract search terms                     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ dataset_download_service.search_all_sources()â”‚
â”‚                                              â”‚
â”‚ 1. Optimize query (fix typos)               â”‚
â”‚ 2. Search Kaggle API (parallel)             â”‚
â”‚ 3. Search HuggingFace API (parallel)        â”‚
â”‚ 4. Rank by semantic relevance               â”‚
â”‚ 5. Return top 5 combined                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Response to User                             â”‚
â”‚                                              â”‚
â”‚ - Top 5 datasets with metadata               â”‚
â”‚ - Relevance scores                           â”‚
â”‚ - Download counts                            â”‚
â”‚ - Source breakdown (Kaggle/HF)              â”‚
â”‚ - Downloadable dataset cards                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Dataset Inspection Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User clicks     â”‚ "Inspect Dataset" or "Add to Collection"
â”‚ dataset card    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ datasets.py: /api/datasets/add-from-kaggle  â”‚
â”‚                                              â”‚
â”‚ 1. Check if Kaggle API configured            â”‚
â”‚ 2. Download dataset to local storage         â”‚
â”‚ 3. Find CSV files in download                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dataset Processing (pandas)                  â”‚
â”‚                                              â”‚
â”‚ 1. Load CSV (first 1000 rows for metadata)  â”‚
â”‚ 2. Generate schema:                          â”‚
â”‚    - Column names                            â”‚
â”‚    - Data types (int, float, object)        â”‚
â”‚    - Null counts                             â”‚
â”‚    - Unique counts                           â”‚
â”‚ 3. Extract sample data (20 rows)            â”‚
â”‚ 4. Auto-detect target column:               â”‚
â”‚    - Match keywords (price, label, etc.)    â”‚
â”‚    - Check user query                        â”‚
â”‚    - Fallback to last column                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save to MongoDB: datasets collection         â”‚
â”‚                                              â”‚
â”‚ {                                            â”‚
â”‚   user_id, name, file_name,                  â”‚
â”‚   row_count, column_count, file_size,        â”‚
â”‚   schema: [...],                             â”‚
â”‚   sample_data: [...],                        â”‚
â”‚   target_column: "detected_column",          â”‚
â”‚   csv_content: "..." (production mode),      â”‚
â”‚   status: "ready"                            â”‚
â”‚ }                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Dataset Card Updated               â”‚
â”‚                                              â”‚
â”‚ âœ“ Shows schema preview                       â”‚
â”‚ âœ“ Shows target column                        â”‚
â”‚ âœ“ "Train Model" button enabled               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Training Flow (Current Implementation)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User initiates  â”‚ Click "Train Model" button
â”‚ training        â”‚ URL: /?chat=XXX&dataset=YYY&train=true
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Chat.tsx (useEffect)                           â”‚
â”‚                                                           â”‚
â”‚ 1. Parse URL params:                                     â”‚
â”‚    - chat_id                                             â”‚
â”‚    - dataset_id                                          â”‚
â”‚    - train=true flag                                     â”‚
â”‚ 2. Call: handleStartTraining(dataset_id, chat_id)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: useTraining Hook                               â”‚
â”‚                                                           â”‚
â”‚ const startTraining = (datasetId, chatId, onProgress) => â”‚
â”‚                                                           â”‚
â”‚ 1. Establish SSE connection:                             â”‚
â”‚    EventSource(`/api/automl/train/${datasetId}?          â”‚
â”‚                 chat_id=${chatId}`)                      â”‚
â”‚                                                           â”‚
â”‚ 2. Listen for events:                                    â”‚
â”‚    - onmessage: Parse JSON data                          â”‚
â”‚    - Handle event types: status, progress, error,        â”‚
â”‚      complete                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ HTTP GET (SSE Stream)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend: automl.py                                        â”‚
â”‚ POST /api/automl/train/{dataset_id}                      â”‚
â”‚                                                           â”‚
â”‚ 1. Validate dataset exists                               â”‚
â”‚ 2. Validate chat exists                                  â”‚
â”‚ 3. Return StreamingResponse(event_generator())           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ async def event_generator(dataset_id, chat_id):          â”‚
â”‚                                                           â”‚
â”‚ PHASE 1: Load Dataset                                    â”‚
â”‚ â”œâ”€ Check dataset metadata in MongoDB                     â”‚
â”‚ â”œâ”€ Verify target_column is set                           â”‚
â”‚ â”œâ”€ Priority 1: Load from csv_content (MongoDB)           â”‚
â”‚ â”‚  â””â”€ Production-safe, works on ephemeral FS             â”‚
â”‚ â”œâ”€ Priority 2: Load from file_path (Development)         â”‚
â”‚ â”‚  â””â”€ Read CSV from disk                                 â”‚
â”‚ â””â”€ Send SSE: "ğŸ“Š Dataset loaded: X rows, Y columns"      â”‚
â”‚                                                           â”‚
â”‚ PHASE 2: Initialize AutoML                               â”‚
â”‚ â”œâ”€ Try: from autogluon.tabular import TabularPredictor  â”‚
â”‚ â”œâ”€ If success: use_autogluon = True                      â”‚
â”‚ â””â”€ If ImportError: use_autogluon = False (simulation)    â”‚
â”‚                                                           â”‚
â”‚ PHASE 3: Data Preparation                                â”‚
â”‚ â”œâ”€ Verify target column exists in DataFrame              â”‚
â”‚ â”œâ”€ Get first 5 rows as sample                            â”‚
â”‚ â”œâ”€ Clean NaN values for JSON serialization               â”‚
â”‚ â””â”€ Save sample to chat as message                        â”‚
â”‚                                                           â”‚
â”‚ PHASE 4A: Real AutoML Training (if available)            â”‚
â”‚ â”œâ”€ Detect task type:                                     â”‚
â”‚ â”‚  â””â”€ Classification: if dtype=object or nunique < 20   â”‚
â”‚ â”‚  â””â”€ Regression: otherwise                              â”‚
â”‚ â”œâ”€ Create TabularPredictor:                              â”‚
â”‚ â”‚  â””â”€ label=target_column                                â”‚
â”‚ â”‚  â””â”€ path=backend/models/{dataset_id}                   â”‚
â”‚ â”‚  â””â”€ problem_type='multiclass' or 'regression'          â”‚
â”‚ â”‚  â””â”€ eval_metric='accuracy' or 'r2'                     â”‚
â”‚ â”œâ”€ Train in executor (non-blocking):                     â”‚
â”‚ â”‚  â””â”€ predictor.fit(train_data=df, time_limit=60,       â”‚
â”‚ â”‚                    presets='medium_quality')           â”‚
â”‚ â”œâ”€ Get leaderboard and best model                        â”‚
â”‚ â””â”€ Calculate metrics:                                    â”‚
â”‚    â””â”€ Classification: accuracy, f1, precision, recall    â”‚
â”‚    â””â”€ Regression: r2_score, MAE, RMSE                    â”‚
â”‚                                                           â”‚
â”‚ PHASE 4B: Simulation Training (fallback)                 â”‚
â”‚ â”œâ”€ Simulate 5 model training steps:                      â”‚
â”‚ â”‚  1. Random Forest (20% progress)                       â”‚
â”‚ â”‚  2. XGBoost (40% progress)                             â”‚
â”‚ â”‚  3. LightGBM (60% progress)                            â”‚
â”‚ â”‚  4. Neural Network (80% progress)                      â”‚
â”‚ â”‚  5. Ensemble (100% progress)                           â”‚
â”‚ â”œâ”€ Each step: send SSE + save message to chat           â”‚
â”‚ â””â”€ Generate random metrics (75-95% accuracy)             â”‚
â”‚                                                           â”‚
â”‚ PHASE 5: Save Model to Database                          â”‚
â”‚ â”œâ”€ Create model document:                                â”‚
â”‚ â”‚  {                                                      â”‚
â”‚ â”‚    user_id, name, base_model,                          â”‚
â”‚ â”‚    dataset_id, task_type,                              â”‚
â”‚ â”‚    metrics: {accuracy, f1, etc.},                      â”‚
â”‚ â”‚    model_path: "backend/models/...",                   â”‚
â”‚ â”‚    uses_real_model: true/false,                        â”‚
â”‚ â”‚    status: "ready"                                     â”‚
â”‚ â”‚  }                                                      â”‚
â”‚ â”œâ”€ Insert into models collection                         â”‚
â”‚ â””â”€ Get model_id                                          â”‚
â”‚                                                           â”‚
â”‚ PHASE 6: Complete & Notify                               â”‚
â”‚ â”œâ”€ Format result message with metrics                    â”‚
â”‚ â”œâ”€ Save final message to chat                            â”‚
â”‚ â””â”€ Send SSE complete event:                              â”‚
â”‚    {                                                      â”‚
â”‚      type: "complete",                                   â”‚
â”‚      message: "Training Complete!",                      â”‚
â”‚      model_id, best_model, metrics                       â”‚
â”‚    }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ SSE Events Stream
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Event Handlers                                 â”‚
â”‚                                                           â”‚
â”‚ eventSource.onmessage = (event) => {                     â”‚
â”‚   const data = JSON.parse(event.data);                   â”‚
â”‚                                                           â”‚
â”‚   switch(data.type) {                                    â”‚
â”‚     case "status":                                       â”‚
â”‚       â””â”€ addMessage(data.message)                        â”‚
â”‚     case "progress":                                     â”‚
â”‚       â””â”€ updateProgressBar(data.progress)                â”‚
â”‚     case "error":                                        â”‚
â”‚       â””â”€ showError(data.message)                         â”‚
â”‚     case "complete":                                     â”‚
â”‚       â”œâ”€ eventSource.close()                             â”‚
â”‚       â”œâ”€ addMessage(data.message)                        â”‚
â”‚       â”œâ”€ updateURL to remove ?train=true                 â”‚
â”‚       â””â”€ showToast("Training Complete!")                 â”‚
â”‚   }                                                       â”‚
â”‚ }                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Code Implementation

### File Structure

```
backend/app/
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ automl.py           â­ Main training endpoint
â”‚   â”œâ”€â”€ messages.py         ğŸ“¨ Chat & query analysis
â”‚   â”œâ”€â”€ datasets.py         ğŸ“¦ Dataset inspection
â”‚   â””â”€â”€ training_jobs.py    ğŸ”„ Background job tracking
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ml_orchestrator.py  ğŸ¤– Model recommendations
â”‚   â”œâ”€â”€ gemini_service.py   âœ¨ AI query analysis
â”‚   â”œâ”€â”€ dataset_download_service.py  ğŸ“¥ Multi-source search
â”‚   â””â”€â”€ huggingface_service.py       ğŸ¤— HF integration
â”‚
â””â”€â”€ models/
    â””â”€â”€ mongodb_models.py   ğŸ—„ï¸ Database schemas

frontend/client/src/
â”œâ”€â”€ pages/
â”‚   â””â”€â”€ Chat.tsx            ğŸ’¬ Main chat interface
â”‚
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useChat.ts          ğŸ“¡ Chat state management
â”‚   â””â”€â”€ useTraining.ts      ğŸ¯ Training SSE handler
â”‚
â””â”€â”€ components/
    â”œâ”€â”€ KaggleDatasetCard.tsx      ğŸ“Š Kaggle dataset UI
    â”œâ”€â”€ HuggingFaceDatasetCard.tsx ğŸ¤— HF dataset UI
    â””â”€â”€ DownloadableDatasetCard.tsx ğŸ“¦ Unified dataset card
```

### Key Code Sections

#### 1. Training Endpoint (`backend/app/routers/automl.py`)

```python
# Line 407-451: Main training endpoint
@router.post("/train/{dataset_id}")
async def train_model(
    dataset_id: str,
    chat_id: str,
    current_user: User = Depends(get_current_user)
):
    """Start AutoML training with SSE progress updates"""

    # Validate dataset and chat
    # Return StreamingResponse with event_generator()

    return StreamingResponse(
        event_generator(dataset_id, chat_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )
```

#### 2. Event Generator (`backend/app/routers/automl.py`)

```python
# Line 17-404: SSE event stream generator
async def event_generator(dataset_id: str, chat_id: str):
    """
    Yields SSE events:
    - data: {"type": "status", "message": "Loading..."}
    - data: {"type": "progress", "message": "Training...", "progress": 50}
    - data: {"type": "complete", "model_id": "...", "metrics": {...}}
    """

    # Phase 1: Load dataset from MongoDB or filesystem
    # Phase 2: Initialize AutoGluon (or fallback to simulation)
    # Phase 3: Prepare data and show sample
    # Phase 4: Train model (real or simulated)
    # Phase 5: Save model to database
    # Phase 6: Send completion event
```

#### 3. Frontend Training Hook (`frontend/client/src/hooks/useTraining.ts`)

```typescript
export const useTraining = () => {
  const [isTraining, setIsTraining] = useState(false);

  const startTraining = (
    datasetId: string,
    chatId: string,
    onProgress: (data: any) => void
  ) => {
    setIsTraining(true);

    // Create SSE connection
    const eventSource = new EventSource(
      `${API_URL}/api/automl/train/${datasetId}?chat_id=${chatId}`
    );

    // Handle events
    eventSource.onmessage = (event) => {
      const data = JSON.parse(event.data);
      onProgress(data);

      if (data.type === 'complete' || data.type === 'error') {
        eventSource.close();
        setIsTraining(false);
      }
    };

    return eventSource;
  };

  return { isTraining, startTraining };
};
```

#### 4. Chat Integration (`frontend/client/src/pages/Chat.tsx`)

```typescript
// Line 29-76: Initialize chat and handle training
useEffect(() => {
  const initializeChat = async () => {
    const urlParams = new URLSearchParams(window.location.search);
    const existingChatId = urlParams.get('chat');
    const datasetId = urlParams.get('dataset');
    const shouldTrain = urlParams.get('train') === 'true';

    if (shouldTrain && datasetId) {
      handleStartTraining(datasetId, existingChatId);
    }
  };

  initializeChat();
}, []);

// Line 79-101: Training handler
const handleStartTraining = (datasetId: string, currentChatId: string) => {
  startTraining(datasetId, currentChatId, (data) => {
    if (data.type === "status" || data.type === "progress") {
      addMessage({
        role: "assistant",
        content: data.message,
        timestamp: new Date()
      });
    } else if (data.type === "complete") {
      addMessage({
        role: "assistant",
        content: data.message,
        timestamp: new Date(),
        metadata: {
          model_id: data.model_id,
          metrics: data.metrics
        }
      });
    }
  });
};
```

---

## ğŸ”— Integration Points

### 1. Dataset Discovery â†’ Training

```
messages.py (line 159-251)
â”œâ”€ Detects: ML task keywords (classify, train, predict)
â”œâ”€ Searches datasets via dataset_download_service
â”œâ”€ Returns metadata with downloadable_datasets
â””â”€ Frontend shows "Add to Collection" button
    â”‚
    â–¼
datasets.py (line 487-678)
â”œâ”€ Downloads dataset from Kaggle/HuggingFace
â”œâ”€ Inspects and generates schema
â”œâ”€ Auto-detects target column
â””â”€ Saves to MongoDB with status="ready"
    â”‚
    â–¼
Chat.tsx (line 270-285)
â””â”€ Shows "Start Fine-tuning" button for ready datasets
    â”‚
    â–¼
automl.py (line 407)
â””â”€ Starts training via SSE
```

### 2. Model Recommendation â†’ Training

```
ml_orchestrator.py (line 28-102)
â”œâ”€ recommend_models(task_description, dataset_id)
â”œâ”€ Searches HuggingFace models
â”œâ”€ Ranks by downloads/likes
â””â”€ Returns top 3 with confidence scores
    â”‚
    â–¼
messages.py (line 255-336)
â”œâ”€ Calls ml_orchestrator when ML task detected
â”œâ”€ Formats recommendations for user
â””â”€ Shows cost/time estimates
    â”‚
    â–¼
automl.py
â””â”€ Uses recommended model for training
```

### 3. Training â†’ Deployment

```
automl.py (line 354-373)
â”œâ”€ Training completes successfully
â”œâ”€ Saves to models collection:
â”‚  {
â”‚    model_id, base_model, metrics,
â”‚    model_path, task_type, status="ready"
â”‚  }
â””â”€ Returns model_id to frontend
    â”‚
    â–¼
training_jobs.py (line 481-540)
â””â”€ deploy_trained_model(job_id)
    â”œâ”€ Creates deployment record
    â”œâ”€ Generates API endpoint
    â””â”€ Returns: /api/deployed/{id}/predict
```

---

## ğŸ“Š Data Flow

### MongoDB Collections

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATASETS Collection                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ {                                                               â”‚
â”‚   _id: ObjectId,                                                â”‚
â”‚   user_id: ObjectId,                                            â”‚
â”‚   name: "House Prices Dataset",                                 â”‚
â”‚   source: "kaggle" | "huggingface" | "upload",                 â”‚
â”‚   kaggle_ref: "username/dataset-name",                          â”‚
â”‚   file_name: "housing.csv",                                     â”‚
â”‚   row_count: 20640,                                             â”‚
â”‚   column_count: 10,                                             â”‚
â”‚   file_size: 1234567,                                           â”‚
â”‚   status: "ready" | "pending" | "error",                       â”‚
â”‚   schema: [                                                     â”‚
â”‚     {name: "longitude", dtype: "float64", null_count: 0},       â”‚
â”‚     {name: "latitude", dtype: "float64", null_count: 0},        â”‚
â”‚     {name: "price", dtype: "float64", null_count: 0}            â”‚
â”‚   ],                                                            â”‚
â”‚   sample_data: [{longitude: -122.23, latitude: 37.88, ...}],   â”‚
â”‚   target_column: "price",                                       â”‚
â”‚   csv_content: "longitude,latitude,price\n..." (production),    â”‚
â”‚   download_path: "./data/kaggle/..." (development),             â”‚
â”‚   uploaded_at: ISODate                                          â”‚
â”‚ }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Used by
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODELS Collection                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ {                                                               â”‚
â”‚   _id: ObjectId,                                                â”‚
â”‚   user_id: ObjectId,                                            â”‚
â”‚   name: "House Prices - WeightedEnsemble",                      â”‚
â”‚   base_model: "WeightedEnsemble_L2",                            â”‚
â”‚   dataset_id: ObjectId (references datasets),                   â”‚
â”‚   task_type: "regression",                                      â”‚
â”‚   status: "ready",                                              â”‚
â”‚   metrics: {                                                    â”‚
â”‚     r2_score: 0.8234,                                           â”‚
â”‚     mae: 45234.56,                                              â”‚
â”‚     rmse: 62345.78                                              â”‚
â”‚   },                                                            â”‚
â”‚   model_path: "backend/models/65f3a.../",                       â”‚
â”‚   uses_real_model: true,                                        â”‚
â”‚   created_at: ISODate                                           â”‚
â”‚ }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Referenced in
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MESSAGES Collection                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ {                                                               â”‚
â”‚   _id: ObjectId,                                                â”‚
â”‚   chat_id: ObjectId,                                            â”‚
â”‚   role: "assistant",                                            â”‚
â”‚   content: "Training Complete! Accuracy: 82.34%",               â”‚
â”‚   query_type: "ml_agent",                                       â”‚
â”‚   metadata: {                                                   â”‚
â”‚     model_id: ObjectId,                                         â”‚
â”‚     metrics: {...},                                             â”‚
â”‚     best_model: "WeightedEnsemble_L2"                           â”‚
â”‚   },                                                            â”‚
â”‚   timestamp: ISODate                                            â”‚
â”‚ }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ API Endpoints

### Training Endpoints

```
POST /api/automl/train/{dataset_id}
â”œâ”€ Query Params: chat_id (required)
â”œâ”€ Auth: Bearer token required
â”œâ”€ Response: text/event-stream (SSE)
â””â”€ Events:
   â”œâ”€ {type: "status", message: "Loading dataset..."}
   â”œâ”€ {type: "progress", message: "Training 1/5", progress: 20}
   â””â”€ {type: "complete", model_id: "...", metrics: {...}}

GET /api/datasets/{dataset_id}
â”œâ”€ Returns: Dataset details with schema and sample data
â””â”€ Used to verify target column before training

POST /api/datasets/inspect
â”œâ”€ Body: {dataset_id, user_query}
â”œâ”€ Downloads dataset if not present
â”œâ”€ Generates schema and detects target column
â””â”€ Updates dataset status to "ready"

POST /api/ml/models/recommend
â”œâ”€ Body: {task_type, dataset_id, constraints}
â”œâ”€ Returns: Top 3 model recommendations with scores
â””â”€ Used before training for model selection
```

### Dataset Discovery Endpoints

```
POST /api/messages/chat
â”œâ”€ Body: {chat_id, content, role: "user"}
â”œâ”€ Analyzes query with Gemini
â”œâ”€ Searches datasets if needed
â””â”€ Returns: AI response with dataset suggestions

POST /api/messages/agent
â”œâ”€ Same as /chat but uses advanced agent mode
â”œâ”€ Direct API search for datasets
â””â”€ Better for explicit dataset requests

POST /api/datasets/search-all
â”œâ”€ Body: {query, optimize_query: true}
â”œâ”€ Searches Kaggle + HuggingFace
â”œâ”€ Ranks by semantic relevance
â””â”€ Returns: {datasets: [...], total_found, kaggle_count, hf_count}
```

---

## ğŸ¯ Training Scenarios

### Scenario 1: Real AutoML Training

```
Prerequisites:
âœ“ AutoGluon installed (pip install autogluon)
âœ“ Dataset with target column selected
âœ“ Sufficient disk space for model cache

Flow:
1. User clicks "Train Model"
2. Backend loads dataset from MongoDB
3. Creates TabularPredictor with AutoGluon
4. Trains for 60 seconds (quick mode)
5. Evaluates models and selects best one
6. Saves model to: backend/models/{dataset_id}/
7. Returns real metrics (accuracy, f1, etc.)

Result:
âœ“ Deployable model saved to disk
âœ“ Real predictions available via API
âœ“ Leaderboard with all models tried
```

### Scenario 2: Simulation Mode Training

```
Prerequisites:
âœ— AutoGluon not installed
âœ“ Dataset with target column selected

Flow:
1. User clicks "Train Model"
2. Backend detects AutoGluon unavailable
3. Simulates 5 training steps (2 seconds each)
4. Generates realistic random metrics
5. No model saved to disk
6. Returns simulated metrics

Result:
âœ“ Demo-ready for UI/UX testing
âœ“ Fast completion (10 seconds)
âœ— No real model for predictions
```

### Scenario 3: Production Training (Ephemeral FS)

```
Prerequisites:
âœ“ Production environment (Render, Heroku, etc.)
âœ“ CSV content stored in MongoDB
âœ“ AutoGluon installed

Flow:
1. Backend loads csv_content from MongoDB
2. Creates DataFrame from in-memory string
3. Trains model (model_path on ephemeral disk)
4. Saves model metadata to MongoDB
5. Model files lost on container restart

Note: For persistent models in production:
- Use S3/GCS to store model files
- Or use model-as-a-service (HuggingFace Inference API)
```

---

## ğŸ” Debugging & Monitoring

### Log Points

```python
# automl.py - Key log points
print(f"[AUTOML] Loading dataset: {dataset_id}")          # Line 26
print(f"[AUTOML] Dataset loaded: {len(df)} rows")         # Line 102
print(f"[AUTOML] AutoML loaded successfully")             # Line 129
print(f"[AUTOML] Using simulation mode")                  # Line 132
print(f"[AUTOML] Training complete: {best_model}")        # Line 233
```

### SSE Event Tracking

```javascript
// Frontend - Track all events
eventSource.onmessage = (event) => {
  console.log('[SSE]', event.data);

  const data = JSON.parse(event.data);
  console.log('[SSE Type]', data.type);
  console.log('[SSE Message]', data.message);
};
```

### MongoDB Queries

```javascript
// Check dataset status
db.datasets.findOne({_id: ObjectId("...")})

// Check trained models
db.models.find({user_id: ObjectId("...")}).sort({created_at: -1})

// Check training messages
db.messages.find({
  chat_id: ObjectId("..."),
  content: {$regex: "Training"}
}).sort({timestamp: -1})
```

---

## ğŸ“ˆ Performance Considerations

### Training Time

| Dataset Size | Model Type | Training Time | Cost (GPU) |
|-------------|-----------|---------------|-----------|
| < 1K rows | AutoML Quick | 1-2 min | $0.01 |
| 1K-10K rows | AutoML Medium | 5-10 min | $0.08 |
| 10K-100K rows | AutoML Full | 30-60 min | $0.50 |
| > 100K rows | Distributed | 2-4 hours | $4.00 |

### Optimization Tips

1. **Dataset Loading**
   - Use `nrows=1000` for inspection
   - Store csv_content in MongoDB for production
   - Use encoding fallback (utf-8 â†’ latin-1 â†’ ignore)

2. **Training**
   - Set `time_limit=60` for quick demos
   - Use `presets='medium_quality'` for balance
   - Run in executor to avoid blocking event loop

3. **Memory Management**
   - Delete DataFrames after use
   - Call `gc.collect()` after training
   - Monitor with `log_memory_usage()`

---

## âœ… Testing Checklist

### Pre-Training Tests
- [ ] Dataset exists in MongoDB
- [ ] Dataset status is "ready"
- [ ] Target column is set
- [ ] CSV content or download_path is available
- [ ] User has permission to access dataset

### During Training Tests
- [ ] SSE connection established
- [ ] Progress events received every 1-2 seconds
- [ ] Messages saved to chat collection
- [ ] Memory usage stays under limit
- [ ] No event loop blocking (health check responds)

### Post-Training Tests
- [ ] Model saved to models collection
- [ ] model_id returned to frontend
- [ ] Metrics are realistic (not NaN or Infinity)
- [ ] Chat shows completion message
- [ ] SSE connection closed properly

---

## ğŸš€ Next Steps

### Immediate Improvements
1. Add model deployment API
2. Implement model predictions endpoint
3. Add training job queue for multiple users
4. Store models in S3/GCS for production

### Future Enhancements
1. Support for image/audio datasets
2. Distributed training for large datasets
3. Hyperparameter tuning with Optuna
4. Model comparison dashboard
5. A/B testing for deployed models

---

## ğŸ“ Support & Resources

- **GitHub:** https://github.com/harshalingle123/smart-ml-assistant
- **AutoGluon Docs:** https://auto.gluon.ai/stable/tutorials/
- **SSE Guide:** https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events

---

**Document Version:** 1.0
**Last Updated:** December 2024
**Status:** Production-Ready âœ…

